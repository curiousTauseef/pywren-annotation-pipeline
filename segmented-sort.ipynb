{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install PyWren-IBM if needed\n",
    "try:\n",
    "    import pywren_ibm_cloud as pywren\n",
    "except ModuleNotFoundError:    \n",
    "    !{sys.executable} -m pip install -U pywren-ibm-cloud==1.0.8\n",
    "    import pywren_ibm_cloud as pywren\n",
    "\n",
    "pywren.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "from ibm_botocore.client import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = json.load(open('config.json'))\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config['ibm_cos']['api_key'],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config['ibm_cos']['endpoint'])\n",
    "bucket = config['pywren']['storage_bucket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data & upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2GB of test data (128M elements * 8 bytes per element * (1 column + 1 index))\n",
    "test_size_mb = 2048\n",
    "n_elements = test_size_mb * 2**20 // 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'val': np.random.rand(n_elements)}, index=np.arange(n_elements))\n",
    "print('df memory usage:',df.memory_usage(index=True).sum() / 2**20, 'MiB')\n",
    "print('df is sorted:', df.val.is_monotonic_increasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by key to simulate unsorted data\n",
    "# Note: Each segment gets repartitioned into pieces that are approximately `segment_size_mb / n_segments` MiB. \n",
    "# Each piece cannot be smaller than 5MiB or else multi-part upload will fail.\n",
    "segment_size_mb = 128\n",
    "n_segments = math.ceil(test_size_mb / segment_size_mb)\n",
    "assert segment_size_mb / n_segments > 5, \"Segments too small - multi-part uploads will fail\"\n",
    "\n",
    "input_keys = [f'segmented_sort/input-{i}.pickle' for i in range(n_segments)]\n",
    "repartition_keys = [f'segmented_sort/repartition-{i}.pickle' for i in range(n_segments)]\n",
    "output_keys = [f'segmented_sort/output-{i}.pickle' for i in range(n_segments)]\n",
    "segm_val_bounds = [(i/n_segments, (i+1)/n_segments) for i in range(n_segments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload\n",
    "for i in range(n_segments):\n",
    "    segment_start = i*n_elements//n_segments\n",
    "    segment_end = (i+1)*n_elements//n_segments\n",
    "    segment_bytes = pickle.dumps(df[segment_start:segment_end].copy())\n",
    "    cos_client.put_object(Bucket=bucket, \n",
    "                          Key=input_keys[i], \n",
    "                          Body=segment_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start multi-part uploads\n",
    "multipart_ids = []\n",
    "for i in range(n_segments):\n",
    "    response = cos_client.create_multipart_upload(\n",
    "        Bucket=config['pywren']['storage_bucket'], \n",
    "        Key=repartition_keys[i])\n",
    "    multipart_ids.append(response['UploadId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition in pywren\n",
    "def repartition_by_val(key, data_stream, ibm_cos, input_i):\n",
    "    input_segment = pickle.loads(data_stream.read())\n",
    "    output_parts = []\n",
    "    for output_i, (lo, hi) in enumerate(segm_val_bounds):\n",
    "        part = ibm_cos.upload_part(\n",
    "            Bucket=bucket, \n",
    "            Key=repartition_keys[output_i], \n",
    "            PartNumber=input_i + 1, \n",
    "            UploadId=multipart_ids[output_i],\n",
    "            Body=input_segment[lambda df: (df.val >= lo) & (df.val < hi)].to_msgpack())\n",
    "        output_parts.append({\n",
    "            \"ETag\": part[\"ETag\"],\n",
    "            \"PartNumber\": input_i + 1,\n",
    "        })\n",
    "    return output_parts\n",
    "\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime='ibmfunctions/action-python-v3.6', runtime_memory=512)\n",
    "iterdata = [[f'{bucket}/{key}', i] for i, key in enumerate(input_keys)]\n",
    "futures = pw.map(repartition_by_val, iterdata)\n",
    "all_output_parts = pw.get_result(futures)\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete multi-part uploads\n",
    "for i, key in enumerate(repartition_keys):\n",
    "    cos_client.complete_multipart_upload(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        UploadId=multipart_ids[i],\n",
    "        MultipartUpload={\n",
    "            \"Parts\": sorted((part_set[i] for part_set in all_output_parts), \n",
    "                            key=lambda part: part['PartNumber'])\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge parts of partitions & sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge & sort new partitions\n",
    "def sort_partition(key, data_stream, ibm_cos, output_i):\n",
    "    partition = pd.concat(pd.read_msgpack(data_stream.read()))\n",
    "    partition.sort_values(by='val', inplace=True)\n",
    "    ibm_cos.put_object(Bucket=bucket, Key=output_keys[output_i], Body=pickle.dumps(partition))\n",
    "\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime_memory=768) \n",
    "iterdata = [[f'{bucket}/{key}', i] for i, key in enumerate(repartition_keys)]\n",
    "futures = pw.map(sort_partition, iterdata)\n",
    "pw.get_result(futures)\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dfs = [pickle.loads(cos_client.get_object(Bucket=bucket, Key=key)['Body'].read()) for key in output_keys]\n",
    "sorted_output = pd.concat(output_dfs)\n",
    "print('output memory usage:', sorted_output.memory_usage(index=True).sum() / 2**20, 'MiB')\n",
    "print('output is sorted:', sorted_output.val.is_monotonic_increasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unfinished multi-part uploads\n",
    "for upload in cos_client.list_multipart_uploads(Bucket=bucket, Prefix='segmented_sort').get('Uploads', []):\n",
    "    print(f'Aborting {upload[\"Key\"]}')\n",
    "    cos_client.abort_multipart_upload(Bucket=bucket, Key=upload['Key'], UploadId=upload['UploadId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temp objects\n",
    "temp_objects = cos_client.list_objects(Bucket=bucket, Prefix='segmented_sort').get('Contents', [])\n",
    "temp_obj_keys = [obj['Key'] for obj in temp_objects]\n",
    "if temp_obj_keys:\n",
    "    print(f'Deleting {temp_obj_keys}')\n",
    "    cos_client.delete_objects(Bucket=bucket, \n",
    "                              Delete={'Objects':[{'Key': key} for key in temp_obj_keys]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug code to list contents of the bucket\n",
    "result = cos_client.list_objects_v2(Bucket=bucket, Prefix='segmented_sort')\n",
    "contents = result.get('Contents', [])\n",
    "total_size = 0\n",
    "out = []\n",
    "for obj in contents:\n",
    "    out.append((obj['Key'], f'{obj[\"Size\"]/2**20}MB', str(obj['LastModified'])))\n",
    "    total_size += obj['Size']\n",
    "print(out)\n",
    "print(f'Total size: {total_size/2**20}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List multi-part uploads\n",
    "cos_client.list_multipart_uploads(Bucket=bucket, Prefix='segmented_sort')['Uploads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-part upload minimum size limit\n",
    "# Conclusion: regardless of total size, all parts except the last must be at least 5 MiB\n",
    "# The last part can be any size.\n",
    "segment_sizes = [5 * 2**20] * 2 + [1]\n",
    "key = 'segmented-sort/multipart-test'\n",
    "test_multipart_id = cos_client.create_multipart_upload(\n",
    "    Bucket=config['pywren']['storage_bucket'], \n",
    "    Key=key)['UploadId']\n",
    "\n",
    "test_parts = []\n",
    "for i, size in enumerate(segment_sizes):\n",
    "    part = cos_client.upload_part(\n",
    "        Bucket=bucket, \n",
    "        Key=key, \n",
    "        PartNumber=i + 1, \n",
    "        UploadId=test_multipart_id,\n",
    "        Body=bytes(size))\n",
    "    test_parts.append({\n",
    "        \"ETag\": part[\"ETag\"],\n",
    "        \"PartNumber\": i + 1,\n",
    "    })\n",
    "\n",
    "# Complete multi-part uploads\n",
    "cos_client.complete_multipart_upload(\n",
    "    Bucket=bucket,\n",
    "    Key=key,\n",
    "    UploadId=test_multipart_id,\n",
    "    MultipartUpload={\n",
    "        \"Parts\": test_parts\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sm]",
   "language": "python",
   "name": "conda-env-sm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
